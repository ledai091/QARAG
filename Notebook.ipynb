{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from accelerate) (0.25.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: filelock in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuADRAGSystem:\n",
    "    def __init__(self, model_name='bert-base-uncased', trained_model_path=None):\n",
    "        self.squad_dataset = self._load_squad_dataset()\n",
    "        \n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if trained_model_path:\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(trained_model_path)\n",
    "        else:\n",
    "            self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        \n",
    "        self.processed_dataset = self._preprocess_squad_dataset()\n",
    "        \n",
    "        self.document_texts = self._extract_context_passages()\n",
    "        self.document_embeddings = self._embed_documents()\n",
    "        self.faiss_index = self._create_faiss_index()\n",
    "        \n",
    "    def _load_squad_dataset(self):\n",
    "        return load_dataset('squad', split='train')\n",
    "    \n",
    "    def _preprocess_squad_dataset(self):\n",
    "        def preprocess_function(examples):\n",
    "            questions = examples['question']\n",
    "            contexts = examples['context']\n",
    "            answers = examples['answers']\n",
    "            \n",
    "            tokenized_examples = self.tokenizer(\n",
    "                questions,\n",
    "                contexts,\n",
    "                truncation=True,\n",
    "                max_length=348,\n",
    "                stride=128,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "            offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "            \n",
    "            tokenized_examples[\"start_positions\"] = []\n",
    "            tokenized_examples[\"end_positions\"] = []\n",
    "            \n",
    "            for i, offsets in enumerate(offset_mapping):\n",
    "                input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "                cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
    "                sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "                \n",
    "                sample_index = sample_mapping[i]\n",
    "                answer = answers[sample_index]\n",
    "                \n",
    "                if len(answer['text']) == 0:\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    start_char = answer['answer_start'][0]\n",
    "                    end_char = start_char + len(answer['text'][0])    \n",
    "                    \n",
    "                    token_start_index = 0\n",
    "                    while sequence_ids[token_start_index] != 1:\n",
    "                        token_start_index += 1\n",
    "                    \n",
    "                    token_end_index = len(input_ids) - 1 \n",
    "                    while sequence_ids[token_end_index] != 1:\n",
    "                        token_end_index -=1\n",
    "                    \n",
    "                    if not(offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                        tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                        tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                    else:\n",
    "                        while (token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char):      \n",
    "                             token_start_index += 1\n",
    "                        tokenized_examples[\"start_positions\"].append(token_start_index -1)\n",
    "                        while (offsets[token_end_index][1] >= end_char):\n",
    "                            token_end_index -= 1\n",
    "                        tokenized_examples[\"end_positions\"].append(token_start_index + 1)\n",
    "            return tokenized_examples\n",
    "        \n",
    "        return self.squad_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.squad_dataset.column_names\n",
    "        )\n",
    "    \n",
    "    def _extract_context_passages(self) -> List[str]:\n",
    "        return list(set(self.squad_dataset['context']))\n",
    "    \n",
    "    def _embed_documents(self) -> np.array:\n",
    "        return np.array(self.embedding_model.encode(self.document_texts))\n",
    "    \n",
    "    def _create_faiss_index(self):\n",
    "         dimension = self.document_embeddings.shape[1]\n",
    "         index = faiss.IndexFlatL2(dimension)\n",
    "         index.add(self.document_embeddings)\n",
    "         return index\n",
    "     \n",
    "    def retrieve_relevant_documents(self, query: str, top_k: int=3) -> List[str]:\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        distances, indices = self.faiss_index.search(query_embedding, top_k)\n",
    "        \n",
    "        return [self.document_texts[i] for i in indices[0]]\n",
    "    \n",
    "    def train_model(self, output_dir='./results', epochs=3):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy='epoch',\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=64,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=0.01,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.processed_dataset,\n",
    "            eval_dataset=self.processed_dataset\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        trainer.save_model(output_dir)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(output_dir)\n",
    "    def generate_answer(self, query:str) -> Dict[str, str]:\n",
    "        relevant_docs = self.retrieve_relevant_documents(query)\n",
    "        context = \" \".join(relevant_docs)\n",
    "        inputs = self.tokenizer(query, context, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        start_logit, end_logits = outputs.start_logits, outputs.end_logits\n",
    "        start_index = torch.argmax(start_logit)\n",
    "        end_index = torch.argmax(end_logits)\n",
    "        \n",
    "        answer_tokens = inputs['input_ids'][0][start_index:end_index+1]\n",
    "        answer = self.tokenizer.decode(answer_tokens)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'context': context,\n",
    "            'confidence': torch.max(start_logit) + torch.max(end_logits)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 87599/87599 [00:32<00:00, 2712.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rag_system = SQuADRAGSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system.train_model(output_dir='./trained_model', epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "        \"Who wrote Harry Potter?\",\n",
    "        \"What is the capital of France?\",\n",
    "        \"When was the United States founded?\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "        result = rag_system.generate_answer(query)\n",
    "        print(f\"\\nQuery: {result['query']}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Context: {result['context'][:200]}...\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
